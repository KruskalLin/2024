---
layout: distill
title: Accelerating Machine Learning Force Fields via Tensor Decomposition
description: 
date: 2025-07-07
future: true
htmlwidgets: true

authors:
 - name: Yuchao Lin
   url: "https://scholar.google.com/citations?user=TRmx8P0AAAAJ&hl=en"
   affiliations:
     name: Lambda Inc.

# must be the exact same name as your blogpost
bibliography: 2025-07-07-TDN-blog.bib

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - please use this format rather than manually creating a markdown table of contents.
toc:
  - name: Equivariance Challenge of MLFF
  - name: The Canonical Recipe Clebsch-Gordan Tensor Product
  - name: Speeding Up Tensor Product via Tensor Decomposition
  - name: Error-Speed Trade-offs and Large-Scale Molecular Study
  - name: What's next

# Below is an example of injecting additional post-specific styles.
# This is used in the 'Layouts' section of this post.
# If you use this post as a template, delete this _styles block.
_styles: >
  
  .center {
      display: block;
      margin-left: auto;
      margin-right: auto;
  }

  .framed {
    border: 1px var(--global-text-color) dashed !important;
    padding: 20px;
  }

  d-article {
    overflow-x: visible;
  }

  .underline {
    text-decoration: underline;
  }

  .todo{
      display: block;
      margin: 12px 0;
      font-style: italic;
      color: red;
  }
  .todo:before {
      content: "TODO: ";
      font-weight: bold;
      font-style: normal;
  }
  summary {
    color: steelblue;
    font-weight: bold;
  }

  summary-math {
    text-align:center;
    color: black
  }

  [data-theme="dark"] summary-math {
    text-align:center;
    color: white
  }

  details[open] {
  --bg: #e2edfc;
  color: black;
  border-radius: 15px;
  padding-left: 8px;
  background: var(--bg);
  outline: 0.5rem solid var(--bg);
  margin: 0 0 2rem 0;
  font-size: 80%;
  line-height: 1.4;
  }

  [data-theme="dark"] details[open] {
  --bg: #112f4a;
  color: white;
  border-radius: 15px;
  padding-left: 8px;
  background: var(--bg);
  outline: 0.5rem solid var(--bg);
  margin: 0 0 2rem 0;
  font-size: 80%;
  }
  .box-note, .box-warning, .box-error, .box-important {
    padding: 15px 15px 15px 10px;
    margin: 20px 20px 20px 5px;
    border: 1px solid #eee;
    border-left-width: 5px;
    border-radius: 5px 3px 3px 5px;
  }
  d-article .box-note {
    background-color: #eee;
    border-left-color: #2980b9;
  }
  d-article .box-warning {
    background-color: #fdf5d4;
    border-left-color: #f1c40f;
  }
  d-article .box-error {
    background-color: #f4dddb;
    border-left-color: #c0392b;
  }
  d-article .box-important {
    background-color: #d4f4dd;
    border-left-color: #2bc039;
  }
  html[data-theme='dark'] d-article .box-note {
    background-color: #555555;
    border-left-color: #2980b9;
  }
  html[data-theme='dark'] d-article .box-warning {
    background-color: #7f7f00;
    border-left-color: #f1c40f;
  }
  html[data-theme='dark'] d-article .box-error {
    background-color: #800000;
    border-left-color: #c0392b;
  }
  html[data-theme='dark'] d-article .box-important {
    background-color: #006600;
    border-left-color: #2bc039;
  }
  d-article aside {
    border: 1px solid #aaa;
    border-radius: 4px;
    padding: .5em .5em 0;
    font-size: 90%;
  }
  .caption { 
    font-size: 80%;
    line-height: 1.2;
    text-align: left;
  }
---

<div style="display: none">
$$
\definecolor{input}{rgb}{0.42, 0.55, 0.74}
\definecolor{params}{rgb}{0.51,0.70,0.40}
\definecolor{output}{rgb}{0.843, 0.608, 0}
\def\mba{\boldsymbol a}
\def\mbb{\boldsymbol b}
\def\mbc{\boldsymbol c}
\def\mbd{\boldsymbol d}
\def\mbe{\boldsymbol e}
\def\mbf{\boldsymbol f}
\def\mbg{\boldsymbol g}
\def\mbh{\boldsymbol h}
\def\mbi{\boldsymbol i}
\def\mbj{\boldsymbol j}
\def\mbk{\boldsymbol k}
\def\mbl{\boldsymbol l}
\def\mbm{\boldsymbol m}
\def\mbn{\boldsymbol n}
\def\mbo{\boldsymbol o}
\def\mbp{\boldsymbol p}
\def\mbq{\boldsymbol q}
\def\mbr{\boldsymbol r}
\def\mbs{\boldsymbol s}
\def\mbt{\boldsymbol t}
\def\mbu{\boldsymbol u}
\def\mbv{\boldsymbol v}
\def\mbw{\textcolor{params}{\boldsymbol w}}
\def\mbx{\textcolor{input}{\boldsymbol x}}
\def\mby{\boldsymbol y}
\def\mbz{\boldsymbol z}
\def\mbA{\boldsymbol A}
\def\mbB{\boldsymbol B}
\def\mbE{\boldsymbol E}
\def\mbH{\boldsymbol{H}}
\def\mbK{\boldsymbol{K}}
\def\mbP{\boldsymbol{P}}
\def\mbR{\boldsymbol{R}}
\def\mbW{\textcolor{params}{\boldsymbol W}}
\def\mbQ{\boldsymbol{Q}}
\def\mbV{\boldsymbol{V}}
\def\mbtheta{\textcolor{params}{\boldsymbol \theta}}
\def\mbzero{\boldsymbol 0}
\def\mbI{\boldsymbol I}
\def\cF{\mathcal F}
\def\cH{\mathcal H}
\def\cL{\mathcal L}
\def\cM{\mathcal M}
\def\cN{\mathcal N}
\def\cX{\mathcal X}
\def\cY{\mathcal Y}
\def\cU{\mathcal U}
\def\bbR{\mathbb R}
\def\y{\textcolor{output}{y}}
$$
</div>


A machine-learning force field (MLFF) is a learned surrogate for quantum mechanics: you give it a 3D molecular structure, and it predicts the **total energy** and the **force on each atom**. This turns the expensive inner loop of atomistic simulation evaluating energies and forces thousands or millions of times into fast neural inference. When MLFFs are accurate and stable, they unlock longer and larger simulations for drug discovery (conformational dynamics, binding), materials design (electrolytes, catalysts), and process chemistry (reaction pathways), shifting work from supercomputers to commodity GPUs. Over the last few years, MLFFs have matured from “promising” to state-of-the-art on many benchmarks, with reviews chronicling how they frequently achieve sub-kcal/mol energy errors while rivaling or beating classical force fields in fidelity.



## Equivariance Challenge of MLFF


{% include figure.html path="assets/img/2025-07-07-TDN-blog/equivariance.png" class="img-fluid" %}

A central challenge in building reliable ML force fields is preserving equivariance. Unlike a vanilla graph neural network, a physically faithful model must respect symmetry by design: If you rotate a molecule in 3D space, its **energy** must stay the same (**rotation invariance**), and its **force** vectors must rotate exactly with the molecule (**rotation equivariance**). Formally, for the rotation group SO(3), or more completely the Euclidean group SE(3), which includes translations, the model should guarantee that rotating the inputs predictably rotates the vector-valued outputs while leaving scalar outputs unchanged. This isn’t just aesthetic correctness; it’s the difference between a network that memorizes poses and one that understands geometry.

Equivariance matters because it bakes physics into the architecture instead of hoping data augmentation will teach it. With only augmentations, the network sees many rotated copies and tries to learn the rule implicitly; with an **equivariant** design, the rule is hard-wired, so every layer preserves the symmetry exactly. In practice, this gives sharper generalization to new orientations and conformations, better data efficiency (you need fewer labeled geometries), and more stable molecular dynamics, because forces remain consistent as structures tumble and vibrate. Empirically, SE(3)/SO(3)-equivariant message passing has repeatedly shown improved accuracy and robustness across molecules, crystals, and surfaces<d-cite key="brehmer2024does,wood2025family,batzner20223,liao2022equiformer"></d-cite>.



## The Canonical Recipe: Clebsch-Gordan Tensor Product

To build an SE(3)/SO(3)-equivariant neural network, we will discuss the canonical recipe of equivariant operations: the **Clebsch-Gordan (CG) tensor product**. Before we get into details of this, we give a gental introduction to spherical harmonics. (Real) spherical harmonics $Y^\ell_m:S^2\to \mathbb{R}$ are functions defined on the surface of a sphere expanding spherical function $f:S^2\to \mathbb{R}$ as the angular Fourier expansion $f(x) = \sum_{\ell \ge 0}\sum_{m=-\ell}^\ell c_{\ell m} Y^\ell_m (x)$, i.e., any direcitonal signal on the sphere can be written as a weighted sum of these spherical harmonics on the unit sphere. The index $\ell$ (degree) sets how many bands/patches as the higher $\ell$, the finer the angular detail; while $m$ (order) rotates and slices that group of spherical harmonics around the axis. Low $\ell$ terms are very smooth (the average and a dipole), and higher $\ell$ terms add progressively finer structure, just like adding higher frequencies in a Fourier series. Practically, spherical harmonics give a compact, rotation-aware coordinate system for directions. If we have a function $f$ on the sphere, expanding it in harmonics is an angular Fourier series, as a few low-$\ell$ coefficients capture coarse trends, and more coefficients add detail.

For a concrete representation of spherical harmonics, let a unit vector $u = (x,y,z)$ and the corresponding spherical harmonics are polynomials of $x,y,z$ such that


$$
Y^0_0(u) = \frac{1}{\sqrt{4\pi}}
$$

$$
Y^1_{-1}(u) = \sqrt{\frac{3}{4\pi}}x, \quad Y^1_0(u) = \sqrt{\frac{3}{4\pi }}y, \quad Y^{1}_{-1}(u)=\sqrt{\frac{3}{4\pi}}z
$$

$$
Y^2_{-2}(u) = \sqrt{\frac{15}{4\pi}}xz, \quad Y^2_{-1}(u) = \sqrt{\frac{15}{4\pi}}xy, \quad Y^2_0(u) = \sqrt{\frac{5}{4\pi}}(y^2 - \frac{x^2 + z^2}{2}), \quad Y^2_1(u) = \sqrt{\frac{15}{4\pi}}yz, \quad Y^2_2(u) = \sqrt{\frac{15}{16\pi}}(z^2 - x^2)
$$

$$
\cdots
$$


In principle, the $$Y^0 = \left(Y^0_0\right)$$ is a scalar invariant to rotation, and $$Y^1 = \left(Y^1_{-1}, Y^1_{0},Y^1_1\right)$$ is a vector equivariant to rotation. Each $Y^\ell$ will act as an element in CG tensor product and is called **irreducible representation** (**irrep**). To incorporate equivariance into models, most modern architectures represent features as irreps of SO(3) and combine them with CG tensor product that are symmetry-correct by construction. CG tensor product fuses irreps while keeping outputs in the right transformation class under rotation. Given spherical harmonics $Y^{\ell_1}$ and $Y^{\ell_2}$, CG tensor product tells us


$$
Y^{\ell_1}\otimes_{CG}Y^{\ell_2} \to \{Y^\ell \}_{\vert\ell_1 - \ell_2\vert\le \ell\le \ell_1 + \ell_2}
$$


that interacting between $$Y^{\ell_1}$$ and $$Y^{\ell_2}$$ can be decomposed into a series of spherical harmonics ranging from $$\vert\ell_1 -\ell_2\vert$$ to $$\ell_1 + \ell_2$$. For example, $$Y^1\otimes_{CG} Y^1$$ can be decomposed into $$Y^0, Y^1, Y^2$$, which are produced by inner product, cross product and traceless part of outer product of $$Y^1$$ and $$Y^1$$. These are all usual equivariant operations over Cartesian space and can be expressed within CG tensor product. Furthermore, each triplet $$(\ell_1, \ell_2, \ell)$$ is called a *path*. And given irreps $u,v$ and the tensor product result $w$, the concrete computation over each path $(\ell_1,\ell_2,\ell)$ is formulated as


$$
w^\ell_m = \sum_{m_1 = -\ell_1}^{\ell_1} \sum_{m=-\ell_2}^{\ell_2} C_{\ell_1,m_1,\ell_2,m_2}^{\ell,m} u^{\ell_1}_{m_1}v^{\ell_2}_{m_2}
$$


with $C_{\ell_1,m_1,\ell_2,m_2}^{\ell,m}$ the **Clebsch-Gordan coefficient** guaranteeing the SO(3) equivariance of the operation.

To model CG tensor product, it is common to set a maximum angular degree $L$ to features. Numerically, computing each path cost $\mathcal O(L^3)$ time complexity, and there exists $O(L^3)$ paths, so the number of “paths” that can couple input irreps grows quickly with the maximum angular degree $L$, and dense implementations scale as $\mathcal{O}(L^6)$. Overall, CG tensor product is elegant but quite computationally expensive.





## Speeding Up Tensor Product via Tensor Decomposition

{% include figure.html path="assets/img/2025-07-07-TDN-blog/tdn.png" class="img-fluid" %}

As we notice, the computation of CG tensor product is time-comsuming. For common numerical computation of tensor product, a direct way to do acceleration is **tensor decomposition**<d-footnote>Tensor decomposition is also called CP (CANDECOMP/PARAFAC) decomposition.</d-footnote><d-cite key="kolda2009tensor"></d-cite>. Inspired by this, we takes a clean, general idea from numerical linear algebra that **approximating a 3-way tensor with a low-rank tensor decomposition** to accelerate the CG tensor product. Given input irreps $u,v$ and output irreps $w$, we can concate all CG coefficients to a single 3-way CG tensor $C$ that maps “left irreps × right irreps” to “output irreps,”, i.e.,

$$
w = C(u\otimes v)
$$


In principle, $C$ is very sparse and it contains computation of all possible paths leading to a single contraction to the tensor product $u\otimes v$. We then consider tensor decomposition factoring $C$ into a sum of rank-1 components such that


$$
C_{kij} = \sum_{r=1}^R A_{kr}B_{ir}C_{jr}
$$


where $A,B,C$ are factor matrices and $i,j,k$ are matrix indices. Computation then becomes three thin dense matrix multiplications and a Hadamard product instead of one huge sparse contraction, such that


$$
w = A(B^\top u\odot C^\top v)
$$


 Tensor decomposition has been a workhorse across signal processing and chemometrics for decades; its appeal here is that the speed/accuracy trade-off is controlled by the rank $R$. We then build **Tensor Decomposition Networks (TDNs)** by replacing each CG block with a rank-$R$ approximation and adding **path-weight sharing** so multiplicity weights are tied across all CG paths. We further provide theory: a uniform bound on the equivariance error induced by truncating to rank $R$, and a universality result showing that, as $R$ grows, the tensor-decomposition-based layer can approximate any equivariant bilinear map. Conceptually, TDN is a drop-in, “turn the crank” replacement for CG layers that trades a tiny, controlled equivariance error for big speed and memory wins.



## Error–Speed Trade-offs and Large-Scale Molecular Study

**Speed and throughput.** At the **operator level**, we time a single tensor-product block with the same tensor shapes and irreps the network would feed it, and we compare the **exact CG contraction** against the **tensor decomposition** with the $R=7L^2$ policy. This isolates the cost of contraction itself, so no particular dataset loading, no other layers, and you see raw math and memory traffic over random generated samples. That’s where the escalating wall-clock factors come from **$4.0\times, 4.8\times, 15.0\times, 26.7\times, 47.6\times, 83.6\times$** for $L=1\ldots6$. The growth is expected. A dense CG contraction scales roughly like $\mathcal{O}(L^6)$ when you account for the $\mathcal{O}(L^3)$ number of admissible paths and the $\mathcal{O}(L^3)$ work per path. The tensor decomposition alternative reduces the work to three skinny matrix multiplies plus a Hadamard product, which scales like $\mathcal{O}(R\cdot L^2)$. With $R\propto L^2$, the overall work becomes $\mathcal{O}(L^4)$, and the constant factors are friendlier: you replace many small, sparsely indexed reductions with a handful of cache-coherent GEMMs. 

At the **model level**, we swap each CG block for its tensor decomposition counterpart and enable **path-weight sharing**. That changes two things at once. First, layers compute faster (the operator result above); second, the **parameter count collapses** from $\mathcal{O}(c^2L^3)$ to $\mathcal{O}(c^2)$ because multiplicity-space weights are tied across all CG paths. In our apples-to-apples runs comparing to Equiformer<d-cite key="liao2022equiformer"></d-cite> with standard CG tensor product, the end-to-end throughput moves from **470.9 → 583.1 samples/s** at $L=1$, **122.5 → 199.7** at $L=2$, and **48.4 → 142.9** at $L=3$, while the parameters shrink from **8.86M/18.19M/33.43M** to **4.42M** across the same three settings. The constant 4.42M is a direct consequence of path-weight sharing: once multiplicity-space weights no longer multiply by the count of CG paths, increasing $L$ no longer balloons the parameter budget. In computational time, the gains come from three compounding effects: fewer FLOPs from $\mathcal{O}(L^6)\to\mathcal{O}(L^4)$, better **arithmetic intensity** because GEMMs reuse data in on-chip caches, and less **indexing overhead** since the tensor decomposition is contiguous matrix math rather than many tiny, sparsely addressed reductions. The effect grows with $L$, which is exactly where vanilla equivariant models become least practical.

**A massive real-world dataset.** To move beyond toy problems, our study trains on **PubChemQCR**, a newly curated dataset derived from the PubChemQC project—a large quantum-chemistry database built from first-principles calculations. Each snapshot provides atomic numbers, 3D coordinates, the DFT total energy, and the per-atom forces; trajectories span a broad chemical space so the model sees both near-equilibrium and off-equilibrium configurations. On the **Small** split (≈1.5 M snapshots), TDN reports **Energy MAE 4.46–5.01 meV** and **Force MAE ≈26.5–26.9 meV/Å** on validation/test, outperforming a suite of competitive baselines. On the **Full** split (≈105 M snapshots), TDN achieves **Energy MAE 1.50–1.65 meV** and **Force MAE ≈19.5–20.4 meV/Å**. These numbers illustrate the central point: decomposed tensor products can preserve the accuracy practitioners expect while *greatly* improving computational efficiency, which is exactly what large-scale MLFF production needs.



## What's Next

Our low-rank tensor product is plug-and-play, but can be improved regarding *engineering* and *hybridization*. On the engineering side, we can move beyond generic PyTorch ops and build fused **CUDA kernels** that do the whole tensor decomposition step in one pass with shared memory, tensor-core GEMMs, and block-sparse layouts. That cuts kernel launches and data movement (the real time sink), lets us use mixed precision safely, and opens the door to auto-tuning rank $R$ per layer at runtime based on simple calibration batches. On the hybrid side, we can integrated tensor decomposition into existing **fast equivariant tricks**, e.g., pair it with cuEquivariance-style kernels, or with **SO(2) convolution**<d-cite key="passaro2023reducing"></d-cite> or **Gaunt TP with FFT**<d-cite key="luo2024enabling"></d-cite> as long as they can be written in form of tensor product. Together we can push equivariant MLFFs to larger systems and longer trajectories without giving up the speed gains that make them practical.
