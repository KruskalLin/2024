<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://kruskallin.github.io/2024/feed.xml" rel="self" type="application/atom+xml"/><link href="https://kruskallin.github.io/2024/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-06T06:28:06+02:00</updated><id>https://kruskallin.github.io/2024/feed.xml</id><title type="html">ICLR Blogposts 2024</title><subtitle>Home to the 2024 ICLR Blogposts track </subtitle><entry><title type="html">Accelerating Machine Learning Force Fields via Tensor Decomposition</title><link href="https://kruskallin.github.io/2024/blog/TDN-blog/" rel="alternate" type="text/html" title="Accelerating Machine Learning Force Fields via Tensor Decomposition"/><published>2025-07-07T00:00:00+02:00</published><updated>2025-07-07T00:00:00+02:00</updated><id>https://kruskallin.github.io/2024/blog/TDN-blog</id><content type="html" xml:base="https://kruskallin.github.io/2024/blog/TDN-blog/"><![CDATA[<div style="display: none"> $$ \definecolor{input}{rgb}{0.42, 0.55, 0.74} \definecolor{params}{rgb}{0.51,0.70,0.40} \definecolor{output}{rgb}{0.843, 0.608, 0} \def\mba{\boldsymbol a} \def\mbb{\boldsymbol b} \def\mbc{\boldsymbol c} \def\mbd{\boldsymbol d} \def\mbe{\boldsymbol e} \def\mbf{\boldsymbol f} \def\mbg{\boldsymbol g} \def\mbh{\boldsymbol h} \def\mbi{\boldsymbol i} \def\mbj{\boldsymbol j} \def\mbk{\boldsymbol k} \def\mbl{\boldsymbol l} \def\mbm{\boldsymbol m} \def\mbn{\boldsymbol n} \def\mbo{\boldsymbol o} \def\mbp{\boldsymbol p} \def\mbq{\boldsymbol q} \def\mbr{\boldsymbol r} \def\mbs{\boldsymbol s} \def\mbt{\boldsymbol t} \def\mbu{\boldsymbol u} \def\mbv{\boldsymbol v} \def\mbw{\textcolor{params}{\boldsymbol w}} \def\mbx{\textcolor{input}{\boldsymbol x}} \def\mby{\boldsymbol y} \def\mbz{\boldsymbol z} \def\mbA{\boldsymbol A} \def\mbB{\boldsymbol B} \def\mbE{\boldsymbol E} \def\mbH{\boldsymbol{H}} \def\mbK{\boldsymbol{K}} \def\mbP{\boldsymbol{P}} \def\mbR{\boldsymbol{R}} \def\mbW{\textcolor{params}{\boldsymbol W}} \def\mbQ{\boldsymbol{Q}} \def\mbV{\boldsymbol{V}} \def\mbtheta{\textcolor{params}{\boldsymbol \theta}} \def\mbzero{\boldsymbol 0} \def\mbI{\boldsymbol I} \def\cF{\mathcal F} \def\cH{\mathcal H} \def\cL{\mathcal L} \def\cM{\mathcal M} \def\cN{\mathcal N} \def\cX{\mathcal X} \def\cY{\mathcal Y} \def\cU{\mathcal U} \def\bbR{\mathbb R} \def\y{\textcolor{output}{y}} $$ </div> <p>A machine-learning force field (MLFF) is a learned surrogate for quantum mechanics: you give it a 3D molecular structure, and it predicts the <strong>total energy</strong> and the <strong>force on each atom</strong>. This turns the expensive inner loop of atomistic simulation evaluating energies and forces thousands or millions of times into fast neural inference. When MLFFs are accurate and stable, they unlock longer and larger simulations for drug discovery (conformational dynamics, binding), materials design (electrolytes, catalysts), and process chemistry (reaction pathways), shifting work from supercomputers to commodity GPUs. Over the last few years, MLFFs have matured from “promising” to state-of-the-art on many benchmarks, with reviews chronicling how they frequently achieve sub-kcal/mol energy errors while rivaling or beating classical force fields in fidelity.</p> <h2 id="equivariance-challenge-of-mlff">Equivariance Challenge of MLFF</h2> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2025-07-07-TDN-blog/equivariance-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2025-07-07-TDN-blog/equivariance-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2025-07-07-TDN-blog/equivariance-1400.webp"/> <img src="/2024/assets/img/2025-07-07-TDN-blog/equivariance.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>A central challenge in building reliable ML force fields is preserving equivariance. Unlike a vanilla graph neural network, a physically faithful model must respect symmetry by design: If you rotate a molecule in 3D space, its <strong>energy</strong> must stay the same (<strong>rotation invariance</strong>), and its <strong>force</strong> vectors must rotate exactly with the molecule (<strong>rotation equivariance</strong>). Formally, for the rotation group SO(3), or more completely the Euclidean group SE(3), which includes translations, the model should guarantee that rotating the inputs predictably rotates the vector-valued outputs while leaving scalar outputs unchanged. This isn’t just aesthetic correctness; it’s the difference between a network that memorizes poses and one that understands geometry.</p> <p>Equivariance matters because it bakes physics into the architecture instead of hoping data augmentation will teach it. With only augmentations, the network sees many rotated copies and tries to learn the rule implicitly; with an <strong>equivariant</strong> design, the rule is hard-wired, so every layer preserves the symmetry exactly. In practice, this gives sharper generalization to new orientations and conformations, better data efficiency (you need fewer labeled geometries), and more stable molecular dynamics, because forces remain consistent as structures tumble and vibrate. Empirically, SE(3)/SO(3)-equivariant message passing has repeatedly shown improved accuracy and robustness across molecules, crystals, and surfaces<d-cite key="brehmer2024does,wood2025family,batzner20223,liao2022equiformer"></d-cite>.</p> <h2 id="the-canonical-recipe-clebsch-gordan-tensor-product">The Canonical Recipe: Clebsch-Gordan Tensor Product</h2> <p>To build an SE(3)/SO(3)-equivariant neural network, we will discuss the canonical recipe of equivariant operations: the <strong>Clebsch-Gordan (CG) tensor product</strong>. Before we get into details of this, we give a gental introduction to spherical harmonics. (Real) spherical harmonics $Y^\ell_m:S^2\to \mathbb{R}$ are functions defined on the surface of a sphere expanding spherical function $f:S^2\to \mathbb{R}$ as the angular Fourier expansion $f(x) = \sum_{\ell \ge 0}\sum_{m=-\ell}^\ell c_{\ell m} Y^\ell_m (x)$, i.e., any direcitonal signal on the sphere can be written as a weighted sum of these spherical harmonics on the unit sphere. The index $\ell$ (degree) sets how many bands/patches as the higher $\ell$, the finer the angular detail; while $m$ (order) rotates and slices that group of spherical harmonics around the axis. Low $\ell$ terms are very smooth (the average and a dipole), and higher $\ell$ terms add progressively finer structure, just like adding higher frequencies in a Fourier series. Practically, spherical harmonics give a compact, rotation-aware coordinate system for directions. If we have a function $f$ on the sphere, expanding it in harmonics is an angular Fourier series, as a few low-$\ell$ coefficients capture coarse trends, and more coefficients add detail.</p> <p>For a concrete representation of spherical harmonics, let a unit vector $u = (x,y,z)$ and the corresponding spherical harmonics are polynomials of $x,y,z$ such that</p> \[Y^0_0(u) = \frac{1}{\sqrt{4\pi}}\] \[Y^1_{-1}(u) = \sqrt{\frac{3}{4\pi}}x, \quad Y^1_0(u) = \sqrt{\frac{3}{4\pi }}y, \quad Y^{1}_{-1}(u)=\sqrt{\frac{3}{4\pi}}z\] \[Y^2_{-2}(u) = \sqrt{\frac{15}{4\pi}}xz, \quad Y^2_{-1}(u) = \sqrt{\frac{15}{4\pi}}xy, \quad Y^2_0(u) = \sqrt{\frac{5}{4\pi}}(y^2 - \frac{x^2 + z^2}{2}), \quad Y^2_1(u) = \sqrt{\frac{15}{4\pi}}yz, \quad Y^2_2(u) = \sqrt{\frac{15}{16\pi}}(z^2 - x^2)\] \[\cdots\] <p>In principle, the \(Y^0 = \left(Y^0_0\right)\) is a scalar invariant to rotation, and \(Y^1 = \left(Y^1_{-1}, Y^1_{0},Y^1_1\right)\) is a vector equivariant to rotation. Each $Y^\ell$ will act as an element in CG tensor product and is called <strong>irreducible representation</strong> (<strong>irrep</strong>). To incorporate equivariance into models, most modern architectures represent features as irreps of SO(3) and combine them with CG tensor product that are symmetry-correct by construction. CG tensor product fuses irreps while keeping outputs in the right transformation class under rotation. Given spherical harmonics $Y^{\ell_1}$ and $Y^{\ell_2}$, CG tensor product tells us</p> \[Y^{\ell_1}\otimes_{CG}Y^{\ell_2} \to \{Y^\ell \}_{\vert\ell_1 - \ell_2\vert\le \ell\le \ell_1 + \ell_2}\] <p>that interacting between \(Y^{\ell_1}\) and \(Y^{\ell_2}\) can be decomposed into a series of spherical harmonics ranging from \(\vert\ell_1 -\ell_2\vert\) to \(\ell_1 + \ell_2\). For example, \(Y^1\otimes_{CG} Y^1\) can be decomposed into \(Y^0, Y^1, Y^2\), which are produced by inner product, cross product and traceless part of outer product of \(Y^1\) and \(Y^1\). These are all usual equivariant operations over Cartesian space and can be expressed within CG tensor product. Furthermore, each triplet \((\ell_1, \ell_2, \ell)\) is called a <em>path</em>. And given irreps $u,v$ and the tensor product result $w$, the concrete computation over each path $(\ell_1,\ell_2,\ell)$ is formulated as</p> \[w^\ell_m = \sum_{m_1 = -\ell_1}^{\ell_1} \sum_{m=-\ell_2}^{\ell_2} C_{\ell_1,m_1,\ell_2,m_2}^{\ell,m} u^{\ell_1}_{m_1}v^{\ell_2}_{m_2}\] <p>with $C_{\ell_1,m_1,\ell_2,m_2}^{\ell,m}$ the <strong>Clebsch-Gordan coefficient</strong> guaranteeing the SO(3) equivariance of the operation.</p> <p>To model CG tensor product, it is common to set a maximum angular degree $L$ to features. Numerically, computing each path cost $\mathcal O(L^3)$ time complexity, and there exists $O(L^3)$ paths, so the number of “paths” that can couple input irreps grows quickly with the maximum angular degree $L$, and dense implementations scale as $\mathcal{O}(L^6)$. Overall, CG tensor product is elegant but quite computationally expensive.</p> <h2 id="speeding-up-tensor-product-via-tensor-decomposition">Speeding Up Tensor Product via Tensor Decomposition</h2> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2025-07-07-TDN-blog/tdn-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2025-07-07-TDN-blog/tdn-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2025-07-07-TDN-blog/tdn-1400.webp"/> <img src="/2024/assets/img/2025-07-07-TDN-blog/tdn.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>As we notice, the computation of CG tensor product is time-comsuming. For common numerical computation of tensor product, a direct way to do acceleration is <strong>tensor decomposition</strong><d-footnote>Tensor decomposition is also called CP (CANDECOMP/PARAFAC) decomposition.</d-footnote><d-cite key="kolda2009tensor"></d-cite>. Inspired by this, we takes a clean, general idea from numerical linear algebra that <strong>approximating a 3-way tensor with a low-rank tensor decomposition</strong> to accelerate the CG tensor product. Given input irreps $u,v$ and output irreps $w$, we can concate all CG coefficients to a single 3-way CG tensor $C$ that maps “left irreps × right irreps” to “output irreps,”, i.e.,</p> \[w = C(u\otimes v)\] <p>In principle, $C$ is very sparse and it contains computation of all possible paths leading to a single contraction to the tensor product $u\otimes v$. We then consider tensor decomposition factoring $C$ into a sum of rank-1 components such that</p> \[C_{kij} = \sum_{r=1}^R A_{kr}B_{ir}C_{jr}\] <p>where $A,B,C$ are factor matrices and $i,j,k$ are matrix indices. Computation then becomes three thin dense matrix multiplications and a Hadamard product instead of one huge sparse contraction, such that</p> \[w = A(B^\top u\odot C^\top v)\] <p>Tensor decomposition has been a workhorse across signal processing and chemometrics for decades; its appeal here is that the speed/accuracy trade-off is controlled by the rank $R$. We then build <strong>Tensor Decomposition Networks (TDNs)</strong> by replacing each CG block with a rank-$R$ approximation and adding <strong>path-weight sharing</strong> so multiplicity weights are tied across all CG paths. We further provide theory: a uniform bound on the equivariance error induced by truncating to rank $R$, and a universality result showing that, as $R$ grows, the tensor-decomposition-based layer can approximate any equivariant bilinear map. Conceptually, TDN is a drop-in, “turn the crank” replacement for CG layers that trades a tiny, controlled equivariance error for big speed and memory wins.</p> <h2 id="errorspeed-trade-offs-and-large-scale-molecular-study">Error–Speed Trade-offs and Large-Scale Molecular Study</h2> <p><strong>Speed and throughput.</strong> At the <strong>operator level</strong>, we time a single tensor-product block with the same tensor shapes and irreps the network would feed it, and we compare the <strong>exact CG contraction</strong> against the <strong>tensor decomposition</strong> with the $R=7L^2$ policy. This isolates the cost of contraction itself, so no particular dataset loading, no other layers, and you see raw math and memory traffic over random generated samples. That’s where the escalating wall-clock factors come from <strong>$4.0\times, 4.8\times, 15.0\times, 26.7\times, 47.6\times, 83.6\times$</strong> for $L=1\ldots6$. The growth is expected. A dense CG contraction scales roughly like $\mathcal{O}(L^6)$ when you account for the $\mathcal{O}(L^3)$ number of admissible paths and the $\mathcal{O}(L^3)$ work per path. The tensor decomposition alternative reduces the work to three skinny matrix multiplies plus a Hadamard product, which scales like $\mathcal{O}(R\cdot L^2)$. With $R\propto L^2$, the overall work becomes $\mathcal{O}(L^4)$, and the constant factors are friendlier: you replace many small, sparsely indexed reductions with a handful of cache-coherent GEMMs.</p> <p>At the <strong>model level</strong>, we swap each CG block for its tensor decomposition counterpart and enable <strong>path-weight sharing</strong>. That changes two things at once. First, layers compute faster (the operator result above); second, the <strong>parameter count collapses</strong> from $\mathcal{O}(c^2L^3)$ to $\mathcal{O}(c^2)$ because multiplicity-space weights are tied across all CG paths. In our apples-to-apples runs comparing to Equiformer<d-cite key="liao2022equiformer"></d-cite> with standard CG tensor product, the end-to-end throughput moves from <strong>470.9 → 583.1 samples/s</strong> at $L=1$, <strong>122.5 → 199.7</strong> at $L=2$, and <strong>48.4 → 142.9</strong> at $L=3$, while the parameters shrink from <strong>8.86M/18.19M/33.43M</strong> to <strong>4.42M</strong> across the same three settings. The constant 4.42M is a direct consequence of path-weight sharing: once multiplicity-space weights no longer multiply by the count of CG paths, increasing $L$ no longer balloons the parameter budget. In computational time, the gains come from three compounding effects: fewer FLOPs from $\mathcal{O}(L^6)\to\mathcal{O}(L^4)$, better <strong>arithmetic intensity</strong> because GEMMs reuse data in on-chip caches, and less <strong>indexing overhead</strong> since the tensor decomposition is contiguous matrix math rather than many tiny, sparsely addressed reductions. The effect grows with $L$, which is exactly where vanilla equivariant models become least practical.</p> <p><strong>A massive real-world dataset.</strong> To move beyond toy problems, our study trains on <strong>PubChemQCR</strong>, a newly curated dataset derived from the PubChemQC project—a large quantum-chemistry database built from first-principles calculations. Each snapshot provides atomic numbers, 3D coordinates, the DFT total energy, and the per-atom forces; trajectories span a broad chemical space so the model sees both near-equilibrium and off-equilibrium configurations. On the <strong>Small</strong> split (≈1.5 M snapshots), TDN reports <strong>Energy MAE 4.46–5.01 meV</strong> and <strong>Force MAE ≈26.5–26.9 meV/Å</strong> on validation/test, outperforming a suite of competitive baselines. On the <strong>Full</strong> split (≈105 M snapshots), TDN achieves <strong>Energy MAE 1.50–1.65 meV</strong> and <strong>Force MAE ≈19.5–20.4 meV/Å</strong>. These numbers illustrate the central point: decomposed tensor products can preserve the accuracy practitioners expect while <em>greatly</em> improving computational efficiency, which is exactly what large-scale MLFF production needs.</p> <h2 id="whats-next">What’s Next</h2> <p>Our low-rank tensor product is plug-and-play, but can be improved regarding <em>engineering</em> and <em>hybridization</em>. On the engineering side, we can move beyond generic PyTorch ops and build fused <strong>CUDA kernels</strong> that do the whole tensor decomposition step in one pass with shared memory, tensor-core GEMMs, and block-sparse layouts. That cuts kernel launches and data movement (the real time sink), lets us use mixed precision safely, and opens the door to auto-tuning rank $R$ per layer at runtime based on simple calibration batches. On the hybrid side, we can integrated tensor decomposition into existing <strong>fast equivariant tricks</strong>, e.g., pair it with cuEquivariance-style kernels, or with <strong>SO(2) convolution</strong><d-cite key="passaro2023reducing"></d-cite> or <strong>Gaunt TP with FFT</strong><d-cite key="luo2024enabling"></d-cite> as long as they can be written in form of tensor product. Together we can push equivariant MLFFs to larger systems and longer trajectories without giving up the speed gains that make them practical.</p>]]></content><author><name>Yuchao Lin</name></author><summary type="html"><![CDATA[$$ \definecolor{input}{rgb}{0.42, 0.55, 0.74} \definecolor{params}{rgb}{0.51,0.70,0.40} \definecolor{output}{rgb}{0.843, 0.608, 0} \def\mba{\boldsymbol a} \def\mbb{\boldsymbol b} \def\mbc{\boldsymbol c} \def\mbd{\boldsymbol d} \def\mbe{\boldsymbol e} \def\mbf{\boldsymbol f} \def\mbg{\boldsymbol g} \def\mbh{\boldsymbol h} \def\mbi{\boldsymbol i} \def\mbj{\boldsymbol j} \def\mbk{\boldsymbol k} \def\mbl{\boldsymbol l} \def\mbm{\boldsymbol m} \def\mbn{\boldsymbol n} \def\mbo{\boldsymbol o} \def\mbp{\boldsymbol p} \def\mbq{\boldsymbol q} \def\mbr{\boldsymbol r} \def\mbs{\boldsymbol s} \def\mbt{\boldsymbol t} \def\mbu{\boldsymbol u} \def\mbv{\boldsymbol v} \def\mbw{\textcolor{params}{\boldsymbol w}} \def\mbx{\textcolor{input}{\boldsymbol x}} \def\mby{\boldsymbol y} \def\mbz{\boldsymbol z} \def\mbA{\boldsymbol A} \def\mbB{\boldsymbol B} \def\mbE{\boldsymbol E} \def\mbH{\boldsymbol{H}} \def\mbK{\boldsymbol{K}} \def\mbP{\boldsymbol{P}} \def\mbR{\boldsymbol{R}} \def\mbW{\textcolor{params}{\boldsymbol W}} \def\mbQ{\boldsymbol{Q}} \def\mbV{\boldsymbol{V}} \def\mbtheta{\textcolor{params}{\boldsymbol \theta}} \def\mbzero{\boldsymbol 0} \def\mbI{\boldsymbol I} \def\cF{\mathcal F} \def\cH{\mathcal H} \def\cL{\mathcal L} \def\cM{\mathcal M} \def\cN{\mathcal N} \def\cX{\mathcal X} \def\cY{\mathcal Y} \def\cU{\mathcal U} \def\bbR{\mathbb R} \def\y{\textcolor{output}{y}} $$]]></summary></entry><entry><title type="html">Data and Representation in Machine Learning Drug Discovery</title><link href="https://kruskallin.github.io/2024/blog/causal-inference/" rel="alternate" type="text/html" title="Data and Representation in Machine Learning Drug Discovery"/><published>2025-07-07T00:00:00+02:00</published><updated>2025-07-07T00:00:00+02:00</updated><id>https://kruskallin.github.io/2024/blog/causal-inference</id><content type="html" xml:base="https://kruskallin.github.io/2024/blog/causal-inference/"><![CDATA[<div style="display: none"> $$ \definecolor{input}{rgb}{0.42, 0.55, 0.74} \definecolor{params}{rgb}{0.51,0.70,0.40} \definecolor{output}{rgb}{0.843, 0.608, 0} \def\mba{\boldsymbol a} \def\mbb{\boldsymbol b} \def\mbc{\boldsymbol c} \def\mbd{\boldsymbol d} \def\mbe{\boldsymbol e} \def\mbf{\boldsymbol f} \def\mbg{\boldsymbol g} \def\mbh{\boldsymbol h} \def\mbi{\boldsymbol i} \def\mbj{\boldsymbol j} \def\mbk{\boldsymbol k} \def\mbl{\boldsymbol l} \def\mbm{\boldsymbol m} \def\mbn{\boldsymbol n} \def\mbo{\boldsymbol o} \def\mbp{\boldsymbol p} \def\mbq{\boldsymbol q} \def\mbr{\boldsymbol r} \def\mbs{\boldsymbol s} \def\mbt{\boldsymbol t} \def\mbu{\boldsymbol u} \def\mbv{\boldsymbol v} \def\mbw{\textcolor{params}{\boldsymbol w}} \def\mbx{\textcolor{input}{\boldsymbol x}} \def\mby{\boldsymbol y} \def\mbz{\boldsymbol z} \def\mbA{\boldsymbol A} \def\mbB{\boldsymbol B} \def\mbE{\boldsymbol E} \def\mbH{\boldsymbol{H}} \def\mbK{\boldsymbol{K}} \def\mbP{\boldsymbol{P}} \def\mbR{\boldsymbol{R}} \def\mbW{\textcolor{params}{\boldsymbol W}} \def\mbQ{\boldsymbol{Q}} \def\mbV{\boldsymbol{V}} \def\mbtheta{\textcolor{params}{\boldsymbol \theta}} \def\mbzero{\boldsymbol 0} \def\mbI{\boldsymbol I} \def\cF{\mathcal F} \def\cH{\mathcal H} \def\cL{\mathcal L} \def\cM{\mathcal M} \def\cN{\mathcal N} \def\cX{\mathcal X} \def\cY{\mathcal Y} \def\cU{\mathcal U} \def\bbR{\mathbb R} \def\y{\textcolor{output}{y}} $$ </div> <p>A molecular representation is a formal mapping that converts the physical reality of a chemical compound into a data structure suitable for computation. An ideal representation preserves the information needed to predict relevant observables—reactivity, binding affinity, synthetic accessibility—while discarding superfluous detail so that downstream models remain tractable. In practice, no single encoding is universally optimal. Line notations such as SMILES are compact and easily tokenized, graph structures capture connectivity and stereochemistry, and three‑dimensional coordinates expose the spatial degrees of freedom that dominate non‑covalent interactions. Modern machine‑learning pipelines move fluidly among these layers, translating one form into another as the task demands.</p> <h2 id="basic-molecular-representations">Basic Molecular Representations</h2> <h3 id="smiles">SMILES</h3> <p>The SMILES specification encodes a molecule as a sequence of ASCII tokens that follow depth‑first traversal of its molecular graph. Atoms appear as element symbols surrounded by optional brackets that annotate isotope, charge, stereochemistry, and hydrogen count. Single bonds are implicit; double, triple, and aromatic bonds use the symbols <code class="language-plaintext highlighter-rouge">=</code>, <code class="language-plaintext highlighter-rouge">#</code>, and <code class="language-plaintext highlighter-rouge">:</code>. Branches are enclosed in parentheses, and ring closures are indicated by numerical indices that mark the two atoms joined. Canonicalization algorithms reorder traversal deterministically, giving each structure a unique “canonical SMILES” that enables rapid hashing and duplicate removal. The representation is concise—benzene reduces to the six‑character string <code class="language-plaintext highlighter-rouge">"c1ccccc1"</code>—and readily tokenized for language models, yet its one‑dimensional nature obscures conformational information and allows syntactically valid but chemically impossible strings. Augmentations such as random SMILES enumeration and SELFIES introduce robustness by ensuring all generated strings decode to valid molecules.</p> <h3 id="molecular-topological-graph">Molecular Topological Graph</h3> <p>A topological graph models a molecule as an undirected multigraph $G=(V,E)$ where vertices correspond to atoms and edges to covalent bonds. Vertex attributes typically include atomic number, formal charge, aromaticity, degree, and hybridization state; edge attributes describe bond order, conjugation, and stereochemical parity. The graph abstraction preserves the combinatorial chemistry that governs reaction transforms and scaffold hopping while remaining invariant to translation and rotation. Algorithms for subgraph matching, cycle detection, and fingerprinting operate naturally on this structure, and it forms the backbone input for message‑passing neural networks. Chirality and cis–trans isomerism require additional edge or face labels so that enantiomers do not collapse into the same topology.</p> <h3 id="3d-molecular-structure">3D Molecular Structure</h3> <p>Three‑dimensional structure specifies Cartesian coordinates for each atom, optionally accompanied by partial charges, atomic radii, or multipole moments. A single molecule usually admits a thermodynamic ensemble of conformers whose populations depend on temperature, solvent, and steric constraints. Quantum chemistry and molecular‑mechanics force fields supply potential energies that rank these conformers, making geometry a central variable in docking, binding free‑energy estimation, and property prediction tasks sensitive to shape and electrostatics. Coordinate files appear in formats such as PDB, SDF, and XYZ, each with conventions for unit cell, connectivity, and metadata. Accuracy in 3D comes at computational cost: learning algorithms must be equivariant to rotations and, for flexible molecules, robust to continuous deformations across conformational space.</p>]]></content><author><name>Yuchao Lin</name></author><summary type="html"><![CDATA[$$ \definecolor{input}{rgb}{0.42, 0.55, 0.74} \definecolor{params}{rgb}{0.51,0.70,0.40} \definecolor{output}{rgb}{0.843, 0.608, 0} \def\mba{\boldsymbol a} \def\mbb{\boldsymbol b} \def\mbc{\boldsymbol c} \def\mbd{\boldsymbol d} \def\mbe{\boldsymbol e} \def\mbf{\boldsymbol f} \def\mbg{\boldsymbol g} \def\mbh{\boldsymbol h} \def\mbi{\boldsymbol i} \def\mbj{\boldsymbol j} \def\mbk{\boldsymbol k} \def\mbl{\boldsymbol l} \def\mbm{\boldsymbol m} \def\mbn{\boldsymbol n} \def\mbo{\boldsymbol o} \def\mbp{\boldsymbol p} \def\mbq{\boldsymbol q} \def\mbr{\boldsymbol r} \def\mbs{\boldsymbol s} \def\mbt{\boldsymbol t} \def\mbu{\boldsymbol u} \def\mbv{\boldsymbol v} \def\mbw{\textcolor{params}{\boldsymbol w}} \def\mbx{\textcolor{input}{\boldsymbol x}} \def\mby{\boldsymbol y} \def\mbz{\boldsymbol z} \def\mbA{\boldsymbol A} \def\mbB{\boldsymbol B} \def\mbE{\boldsymbol E} \def\mbH{\boldsymbol{H}} \def\mbK{\boldsymbol{K}} \def\mbP{\boldsymbol{P}} \def\mbR{\boldsymbol{R}} \def\mbW{\textcolor{params}{\boldsymbol W}} \def\mbQ{\boldsymbol{Q}} \def\mbV{\boldsymbol{V}} \def\mbtheta{\textcolor{params}{\boldsymbol \theta}} \def\mbzero{\boldsymbol 0} \def\mbI{\boldsymbol I} \def\cF{\mathcal F} \def\cH{\mathcal H} \def\cL{\mathcal L} \def\cM{\mathcal M} \def\cN{\mathcal N} \def\cX{\mathcal X} \def\cY{\mathcal Y} \def\cU{\mathcal U} \def\bbR{\mathbb R} \def\y{\textcolor{output}{y}} $$]]></summary></entry><entry><title type="html">Molecular Graphs and Geometric Graph Neural Networks</title><link href="https://kruskallin.github.io/2024/blog/de-novo-molecular-design/" rel="alternate" type="text/html" title="Molecular Graphs and Geometric Graph Neural Networks"/><published>2025-07-07T00:00:00+02:00</published><updated>2025-07-07T00:00:00+02:00</updated><id>https://kruskallin.github.io/2024/blog/de-novo-molecular-design</id><content type="html" xml:base="https://kruskallin.github.io/2024/blog/de-novo-molecular-design/"><![CDATA[<div style="display: none"> $$ \definecolor{input}{rgb}{0.42, 0.55, 0.74} \definecolor{params}{rgb}{0.51,0.70,0.40} \definecolor{output}{rgb}{0.843, 0.608, 0} \def\mba{\boldsymbol a} \def\mbb{\boldsymbol b} \def\mbc{\boldsymbol c} \def\mbd{\boldsymbol d} \def\mbe{\boldsymbol e} \def\mbf{\boldsymbol f} \def\mbg{\boldsymbol g} \def\mbh{\boldsymbol h} \def\mbi{\boldsymbol i} \def\mbj{\boldsymbol j} \def\mbk{\boldsymbol k} \def\mbl{\boldsymbol l} \def\mbm{\boldsymbol m} \def\mbn{\boldsymbol n} \def\mbo{\boldsymbol o} \def\mbp{\boldsymbol p} \def\mbq{\boldsymbol q} \def\mbr{\boldsymbol r} \def\mbs{\boldsymbol s} \def\mbt{\boldsymbol t} \def\mbu{\boldsymbol u} \def\mbv{\boldsymbol v} \def\mbw{\textcolor{params}{\boldsymbol w}} \def\mbx{\textcolor{input}{\boldsymbol x}} \def\mby{\boldsymbol y} \def\mbz{\boldsymbol z} \def\mbA{\boldsymbol A} \def\mbB{\boldsymbol B} \def\mbE{\boldsymbol E} \def\mbH{\boldsymbol{H}} \def\mbK{\boldsymbol{K}} \def\mbP{\boldsymbol{P}} \def\mbR{\boldsymbol{R}} \def\mbW{\textcolor{params}{\boldsymbol W}} \def\mbQ{\boldsymbol{Q}} \def\mbV{\boldsymbol{V}} \def\mbtheta{\textcolor{params}{\boldsymbol \theta}} \def\mbzero{\boldsymbol 0} \def\mbI{\boldsymbol I} \def\cF{\mathcal F} \def\cH{\mathcal H} \def\cL{\mathcal L} \def\cM{\mathcal M} \def\cN{\mathcal N} \def\cX{\mathcal X} \def\cY{\mathcal Y} \def\cU{\mathcal U} \def\bbR{\mathbb R} \def\y{\textcolor{output}{y}} $$ </div>]]></content><author><name>Yuchao Lin</name></author><summary type="html"><![CDATA[$$ \definecolor{input}{rgb}{0.42, 0.55, 0.74} \definecolor{params}{rgb}{0.51,0.70,0.40} \definecolor{output}{rgb}{0.843, 0.608, 0} \def\mba{\boldsymbol a} \def\mbb{\boldsymbol b} \def\mbc{\boldsymbol c} \def\mbd{\boldsymbol d} \def\mbe{\boldsymbol e} \def\mbf{\boldsymbol f} \def\mbg{\boldsymbol g} \def\mbh{\boldsymbol h} \def\mbi{\boldsymbol i} \def\mbj{\boldsymbol j} \def\mbk{\boldsymbol k} \def\mbl{\boldsymbol l} \def\mbm{\boldsymbol m} \def\mbn{\boldsymbol n} \def\mbo{\boldsymbol o} \def\mbp{\boldsymbol p} \def\mbq{\boldsymbol q} \def\mbr{\boldsymbol r} \def\mbs{\boldsymbol s} \def\mbt{\boldsymbol t} \def\mbu{\boldsymbol u} \def\mbv{\boldsymbol v} \def\mbw{\textcolor{params}{\boldsymbol w}} \def\mbx{\textcolor{input}{\boldsymbol x}} \def\mby{\boldsymbol y} \def\mbz{\boldsymbol z} \def\mbA{\boldsymbol A} \def\mbB{\boldsymbol B} \def\mbE{\boldsymbol E} \def\mbH{\boldsymbol{H}} \def\mbK{\boldsymbol{K}} \def\mbP{\boldsymbol{P}} \def\mbR{\boldsymbol{R}} \def\mbW{\textcolor{params}{\boldsymbol W}} \def\mbQ{\boldsymbol{Q}} \def\mbV{\boldsymbol{V}} \def\mbtheta{\textcolor{params}{\boldsymbol \theta}} \def\mbzero{\boldsymbol 0} \def\mbI{\boldsymbol I} \def\cF{\mathcal F} \def\cH{\mathcal H} \def\cL{\mathcal L} \def\cM{\mathcal M} \def\cN{\mathcal N} \def\cX{\mathcal X} \def\cY{\mathcal Y} \def\cU{\mathcal U} \def\bbR{\mathbb R} \def\y{\textcolor{output}{y}} $$]]></summary></entry><entry><title type="html">Data and Representation in Machine Learning Drug Discovery</title><link href="https://kruskallin.github.io/2024/blog/de-novo-protein-design/" rel="alternate" type="text/html" title="Data and Representation in Machine Learning Drug Discovery"/><published>2025-07-07T00:00:00+02:00</published><updated>2025-07-07T00:00:00+02:00</updated><id>https://kruskallin.github.io/2024/blog/de-novo-protein-design</id><content type="html" xml:base="https://kruskallin.github.io/2024/blog/de-novo-protein-design/"><![CDATA[<div style="display: none"> $$ \definecolor{input}{rgb}{0.42, 0.55, 0.74} \definecolor{params}{rgb}{0.51,0.70,0.40} \definecolor{output}{rgb}{0.843, 0.608, 0} \def\mba{\boldsymbol a} \def\mbb{\boldsymbol b} \def\mbc{\boldsymbol c} \def\mbd{\boldsymbol d} \def\mbe{\boldsymbol e} \def\mbf{\boldsymbol f} \def\mbg{\boldsymbol g} \def\mbh{\boldsymbol h} \def\mbi{\boldsymbol i} \def\mbj{\boldsymbol j} \def\mbk{\boldsymbol k} \def\mbl{\boldsymbol l} \def\mbm{\boldsymbol m} \def\mbn{\boldsymbol n} \def\mbo{\boldsymbol o} \def\mbp{\boldsymbol p} \def\mbq{\boldsymbol q} \def\mbr{\boldsymbol r} \def\mbs{\boldsymbol s} \def\mbt{\boldsymbol t} \def\mbu{\boldsymbol u} \def\mbv{\boldsymbol v} \def\mbw{\textcolor{params}{\boldsymbol w}} \def\mbx{\textcolor{input}{\boldsymbol x}} \def\mby{\boldsymbol y} \def\mbz{\boldsymbol z} \def\mbA{\boldsymbol A} \def\mbB{\boldsymbol B} \def\mbE{\boldsymbol E} \def\mbH{\boldsymbol{H}} \def\mbK{\boldsymbol{K}} \def\mbP{\boldsymbol{P}} \def\mbR{\boldsymbol{R}} \def\mbW{\textcolor{params}{\boldsymbol W}} \def\mbQ{\boldsymbol{Q}} \def\mbV{\boldsymbol{V}} \def\mbtheta{\textcolor{params}{\boldsymbol \theta}} \def\mbzero{\boldsymbol 0} \def\mbI{\boldsymbol I} \def\cF{\mathcal F} \def\cH{\mathcal H} \def\cL{\mathcal L} \def\cM{\mathcal M} \def\cN{\mathcal N} \def\cX{\mathcal X} \def\cY{\mathcal Y} \def\cU{\mathcal U} \def\bbR{\mathbb R} \def\y{\textcolor{output}{y}} $$ </div> <p>A molecular representation is a formal mapping that converts the physical reality of a chemical compound into a data structure suitable for computation. An ideal representation preserves the information needed to predict relevant observables—reactivity, binding affinity, synthetic accessibility—while discarding superfluous detail so that downstream models remain tractable. In practice, no single encoding is universally optimal. Line notations such as SMILES are compact and easily tokenized, graph structures capture connectivity and stereochemistry, and three‑dimensional coordinates expose the spatial degrees of freedom that dominate non‑covalent interactions. Modern machine‑learning pipelines move fluidly among these layers, translating one form into another as the task demands.</p> <h2 id="basic-molecular-representations">Basic Molecular Representations</h2> <h3 id="smiles">SMILES</h3> <p>The SMILES specification encodes a molecule as a sequence of ASCII tokens that follow depth‑first traversal of its molecular graph. Atoms appear as element symbols surrounded by optional brackets that annotate isotope, charge, stereochemistry, and hydrogen count. Single bonds are implicit; double, triple, and aromatic bonds use the symbols <code class="language-plaintext highlighter-rouge">=</code>, <code class="language-plaintext highlighter-rouge">#</code>, and <code class="language-plaintext highlighter-rouge">:</code>. Branches are enclosed in parentheses, and ring closures are indicated by numerical indices that mark the two atoms joined. Canonicalization algorithms reorder traversal deterministically, giving each structure a unique “canonical SMILES” that enables rapid hashing and duplicate removal. The representation is concise—benzene reduces to the six‑character string <code class="language-plaintext highlighter-rouge">"c1ccccc1"</code>—and readily tokenized for language models, yet its one‑dimensional nature obscures conformational information and allows syntactically valid but chemically impossible strings. Augmentations such as random SMILES enumeration and SELFIES introduce robustness by ensuring all generated strings decode to valid molecules.</p> <h3 id="molecular-topological-graph">Molecular Topological Graph</h3> <p>A topological graph models a molecule as an undirected multigraph $G=(V,E)$ where vertices correspond to atoms and edges to covalent bonds. Vertex attributes typically include atomic number, formal charge, aromaticity, degree, and hybridization state; edge attributes describe bond order, conjugation, and stereochemical parity. The graph abstraction preserves the combinatorial chemistry that governs reaction transforms and scaffold hopping while remaining invariant to translation and rotation. Algorithms for subgraph matching, cycle detection, and fingerprinting operate naturally on this structure, and it forms the backbone input for message‑passing neural networks. Chirality and cis–trans isomerism require additional edge or face labels so that enantiomers do not collapse into the same topology.</p> <h3 id="3d-molecular-structure">3D Molecular Structure</h3> <p>Three‑dimensional structure specifies Cartesian coordinates for each atom, optionally accompanied by partial charges, atomic radii, or multipole moments. A single molecule usually admits a thermodynamic ensemble of conformers whose populations depend on temperature, solvent, and steric constraints. Quantum chemistry and molecular‑mechanics force fields supply potential energies that rank these conformers, making geometry a central variable in docking, binding free‑energy estimation, and property prediction tasks sensitive to shape and electrostatics. Coordinate files appear in formats such as PDB, SDF, and XYZ, each with conventions for unit cell, connectivity, and metadata. Accuracy in 3D comes at computational cost: learning algorithms must be equivariant to rotations and, for flexible molecules, robust to continuous deformations across conformational space.</p>]]></content><author><name>Yuchao Lin</name></author><summary type="html"><![CDATA[$$ \definecolor{input}{rgb}{0.42, 0.55, 0.74} \definecolor{params}{rgb}{0.51,0.70,0.40} \definecolor{output}{rgb}{0.843, 0.608, 0} \def\mba{\boldsymbol a} \def\mbb{\boldsymbol b} \def\mbc{\boldsymbol c} \def\mbd{\boldsymbol d} \def\mbe{\boldsymbol e} \def\mbf{\boldsymbol f} \def\mbg{\boldsymbol g} \def\mbh{\boldsymbol h} \def\mbi{\boldsymbol i} \def\mbj{\boldsymbol j} \def\mbk{\boldsymbol k} \def\mbl{\boldsymbol l} \def\mbm{\boldsymbol m} \def\mbn{\boldsymbol n} \def\mbo{\boldsymbol o} \def\mbp{\boldsymbol p} \def\mbq{\boldsymbol q} \def\mbr{\boldsymbol r} \def\mbs{\boldsymbol s} \def\mbt{\boldsymbol t} \def\mbu{\boldsymbol u} \def\mbv{\boldsymbol v} \def\mbw{\textcolor{params}{\boldsymbol w}} \def\mbx{\textcolor{input}{\boldsymbol x}} \def\mby{\boldsymbol y} \def\mbz{\boldsymbol z} \def\mbA{\boldsymbol A} \def\mbB{\boldsymbol B} \def\mbE{\boldsymbol E} \def\mbH{\boldsymbol{H}} \def\mbK{\boldsymbol{K}} \def\mbP{\boldsymbol{P}} \def\mbR{\boldsymbol{R}} \def\mbW{\textcolor{params}{\boldsymbol W}} \def\mbQ{\boldsymbol{Q}} \def\mbV{\boldsymbol{V}} \def\mbtheta{\textcolor{params}{\boldsymbol \theta}} \def\mbzero{\boldsymbol 0} \def\mbI{\boldsymbol I} \def\cF{\mathcal F} \def\cH{\mathcal H} \def\cL{\mathcal L} \def\cM{\mathcal M} \def\cN{\mathcal N} \def\cX{\mathcal X} \def\cY{\mathcal Y} \def\cU{\mathcal U} \def\bbR{\mathbb R} \def\y{\textcolor{output}{y}} $$]]></summary></entry><entry><title type="html">Molecular Graphs and Geometric Graph Neural Networks</title><link href="https://kruskallin.github.io/2024/blog/docking/" rel="alternate" type="text/html" title="Molecular Graphs and Geometric Graph Neural Networks"/><published>2025-07-07T00:00:00+02:00</published><updated>2025-07-07T00:00:00+02:00</updated><id>https://kruskallin.github.io/2024/blog/docking</id><content type="html" xml:base="https://kruskallin.github.io/2024/blog/docking/"><![CDATA[<div style="display: none"> $$ \definecolor{input}{rgb}{0.42, 0.55, 0.74} \definecolor{params}{rgb}{0.51,0.70,0.40} \definecolor{output}{rgb}{0.843, 0.608, 0} \def\mba{\boldsymbol a} \def\mbb{\boldsymbol b} \def\mbc{\boldsymbol c} \def\mbd{\boldsymbol d} \def\mbe{\boldsymbol e} \def\mbf{\boldsymbol f} \def\mbg{\boldsymbol g} \def\mbh{\boldsymbol h} \def\mbi{\boldsymbol i} \def\mbj{\boldsymbol j} \def\mbk{\boldsymbol k} \def\mbl{\boldsymbol l} \def\mbm{\boldsymbol m} \def\mbn{\boldsymbol n} \def\mbo{\boldsymbol o} \def\mbp{\boldsymbol p} \def\mbq{\boldsymbol q} \def\mbr{\boldsymbol r} \def\mbs{\boldsymbol s} \def\mbt{\boldsymbol t} \def\mbu{\boldsymbol u} \def\mbv{\boldsymbol v} \def\mbw{\textcolor{params}{\boldsymbol w}} \def\mbx{\textcolor{input}{\boldsymbol x}} \def\mby{\boldsymbol y} \def\mbz{\boldsymbol z} \def\mbA{\boldsymbol A} \def\mbB{\boldsymbol B} \def\mbE{\boldsymbol E} \def\mbH{\boldsymbol{H}} \def\mbK{\boldsymbol{K}} \def\mbP{\boldsymbol{P}} \def\mbR{\boldsymbol{R}} \def\mbW{\textcolor{params}{\boldsymbol W}} \def\mbQ{\boldsymbol{Q}} \def\mbV{\boldsymbol{V}} \def\mbtheta{\textcolor{params}{\boldsymbol \theta}} \def\mbzero{\boldsymbol 0} \def\mbI{\boldsymbol I} \def\cF{\mathcal F} \def\cH{\mathcal H} \def\cL{\mathcal L} \def\cM{\mathcal M} \def\cN{\mathcal N} \def\cX{\mathcal X} \def\cY{\mathcal Y} \def\cU{\mathcal U} \def\bbR{\mathbb R} \def\y{\textcolor{output}{y}} $$ </div>]]></content><author><name>Yuchao Lin</name></author><summary type="html"><![CDATA[$$ \definecolor{input}{rgb}{0.42, 0.55, 0.74} \definecolor{params}{rgb}{0.51,0.70,0.40} \definecolor{output}{rgb}{0.843, 0.608, 0} \def\mba{\boldsymbol a} \def\mbb{\boldsymbol b} \def\mbc{\boldsymbol c} \def\mbd{\boldsymbol d} \def\mbe{\boldsymbol e} \def\mbf{\boldsymbol f} \def\mbg{\boldsymbol g} \def\mbh{\boldsymbol h} \def\mbi{\boldsymbol i} \def\mbj{\boldsymbol j} \def\mbk{\boldsymbol k} \def\mbl{\boldsymbol l} \def\mbm{\boldsymbol m} \def\mbn{\boldsymbol n} \def\mbo{\boldsymbol o} \def\mbp{\boldsymbol p} \def\mbq{\boldsymbol q} \def\mbr{\boldsymbol r} \def\mbs{\boldsymbol s} \def\mbt{\boldsymbol t} \def\mbu{\boldsymbol u} \def\mbv{\boldsymbol v} \def\mbw{\textcolor{params}{\boldsymbol w}} \def\mbx{\textcolor{input}{\boldsymbol x}} \def\mby{\boldsymbol y} \def\mbz{\boldsymbol z} \def\mbA{\boldsymbol A} \def\mbB{\boldsymbol B} \def\mbE{\boldsymbol E} \def\mbH{\boldsymbol{H}} \def\mbK{\boldsymbol{K}} \def\mbP{\boldsymbol{P}} \def\mbR{\boldsymbol{R}} \def\mbW{\textcolor{params}{\boldsymbol W}} \def\mbQ{\boldsymbol{Q}} \def\mbV{\boldsymbol{V}} \def\mbtheta{\textcolor{params}{\boldsymbol \theta}} \def\mbzero{\boldsymbol 0} \def\mbI{\boldsymbol I} \def\cF{\mathcal F} \def\cH{\mathcal H} \def\cL{\mathcal L} \def\cM{\mathcal M} \def\cN{\mathcal N} \def\cX{\mathcal X} \def\cY{\mathcal Y} \def\cU{\mathcal U} \def\bbR{\mathbb R} \def\y{\textcolor{output}{y}} $$]]></summary></entry><entry><title type="html">Molecular Graphs and Geometric Graph Neural Networks</title><link href="https://kruskallin.github.io/2024/blog/future-reading/" rel="alternate" type="text/html" title="Molecular Graphs and Geometric Graph Neural Networks"/><published>2025-07-07T00:00:00+02:00</published><updated>2025-07-07T00:00:00+02:00</updated><id>https://kruskallin.github.io/2024/blog/future-reading</id><content type="html" xml:base="https://kruskallin.github.io/2024/blog/future-reading/"><![CDATA[<div style="display: none"> $$ \definecolor{input}{rgb}{0.42, 0.55, 0.74} \definecolor{params}{rgb}{0.51,0.70,0.40} \definecolor{output}{rgb}{0.843, 0.608, 0} \def\mba{\boldsymbol a} \def\mbb{\boldsymbol b} \def\mbc{\boldsymbol c} \def\mbd{\boldsymbol d} \def\mbe{\boldsymbol e} \def\mbf{\boldsymbol f} \def\mbg{\boldsymbol g} \def\mbh{\boldsymbol h} \def\mbi{\boldsymbol i} \def\mbj{\boldsymbol j} \def\mbk{\boldsymbol k} \def\mbl{\boldsymbol l} \def\mbm{\boldsymbol m} \def\mbn{\boldsymbol n} \def\mbo{\boldsymbol o} \def\mbp{\boldsymbol p} \def\mbq{\boldsymbol q} \def\mbr{\boldsymbol r} \def\mbs{\boldsymbol s} \def\mbt{\boldsymbol t} \def\mbu{\boldsymbol u} \def\mbv{\boldsymbol v} \def\mbw{\textcolor{params}{\boldsymbol w}} \def\mbx{\textcolor{input}{\boldsymbol x}} \def\mby{\boldsymbol y} \def\mbz{\boldsymbol z} \def\mbA{\boldsymbol A} \def\mbB{\boldsymbol B} \def\mbE{\boldsymbol E} \def\mbH{\boldsymbol{H}} \def\mbK{\boldsymbol{K}} \def\mbP{\boldsymbol{P}} \def\mbR{\boldsymbol{R}} \def\mbW{\textcolor{params}{\boldsymbol W}} \def\mbQ{\boldsymbol{Q}} \def\mbV{\boldsymbol{V}} \def\mbtheta{\textcolor{params}{\boldsymbol \theta}} \def\mbzero{\boldsymbol 0} \def\mbI{\boldsymbol I} \def\cF{\mathcal F} \def\cH{\mathcal H} \def\cL{\mathcal L} \def\cM{\mathcal M} \def\cN{\mathcal N} \def\cX{\mathcal X} \def\cY{\mathcal Y} \def\cU{\mathcal U} \def\bbR{\mathbb R} \def\y{\textcolor{output}{y}} $$ </div>]]></content><author><name>Yuchao Lin</name></author><summary type="html"><![CDATA[$$ \definecolor{input}{rgb}{0.42, 0.55, 0.74} \definecolor{params}{rgb}{0.51,0.70,0.40} \definecolor{output}{rgb}{0.843, 0.608, 0} \def\mba{\boldsymbol a} \def\mbb{\boldsymbol b} \def\mbc{\boldsymbol c} \def\mbd{\boldsymbol d} \def\mbe{\boldsymbol e} \def\mbf{\boldsymbol f} \def\mbg{\boldsymbol g} \def\mbh{\boldsymbol h} \def\mbi{\boldsymbol i} \def\mbj{\boldsymbol j} \def\mbk{\boldsymbol k} \def\mbl{\boldsymbol l} \def\mbm{\boldsymbol m} \def\mbn{\boldsymbol n} \def\mbo{\boldsymbol o} \def\mbp{\boldsymbol p} \def\mbq{\boldsymbol q} \def\mbr{\boldsymbol r} \def\mbs{\boldsymbol s} \def\mbt{\boldsymbol t} \def\mbu{\boldsymbol u} \def\mbv{\boldsymbol v} \def\mbw{\textcolor{params}{\boldsymbol w}} \def\mbx{\textcolor{input}{\boldsymbol x}} \def\mby{\boldsymbol y} \def\mbz{\boldsymbol z} \def\mbA{\boldsymbol A} \def\mbB{\boldsymbol B} \def\mbE{\boldsymbol E} \def\mbH{\boldsymbol{H}} \def\mbK{\boldsymbol{K}} \def\mbP{\boldsymbol{P}} \def\mbR{\boldsymbol{R}} \def\mbW{\textcolor{params}{\boldsymbol W}} \def\mbQ{\boldsymbol{Q}} \def\mbV{\boldsymbol{V}} \def\mbtheta{\textcolor{params}{\boldsymbol \theta}} \def\mbzero{\boldsymbol 0} \def\mbI{\boldsymbol I} \def\cF{\mathcal F} \def\cH{\mathcal H} \def\cL{\mathcal L} \def\cM{\mathcal M} \def\cN{\mathcal N} \def\cX{\mathcal X} \def\cY{\mathcal Y} \def\cU{\mathcal U} \def\bbR{\mathbb R} \def\y{\textcolor{output}{y}} $$]]></summary></entry><entry><title type="html">Molecular Graphs and Geometric Graph Neural Networks</title><link href="https://kruskallin.github.io/2024/blog/graph-neural-networks/" rel="alternate" type="text/html" title="Molecular Graphs and Geometric Graph Neural Networks"/><published>2025-07-07T00:00:00+02:00</published><updated>2025-07-07T00:00:00+02:00</updated><id>https://kruskallin.github.io/2024/blog/graph-neural-networks</id><content type="html" xml:base="https://kruskallin.github.io/2024/blog/graph-neural-networks/"><![CDATA[<div style="display: none"> $$ \definecolor{input}{rgb}{0.42, 0.55, 0.74} \definecolor{params}{rgb}{0.51,0.70,0.40} \definecolor{output}{rgb}{0.843, 0.608, 0} \def\mba{\boldsymbol a} \def\mbb{\boldsymbol b} \def\mbc{\boldsymbol c} \def\mbd{\boldsymbol d} \def\mbe{\boldsymbol e} \def\mbf{\boldsymbol f} \def\mbg{\boldsymbol g} \def\mbh{\boldsymbol h} \def\mbi{\boldsymbol i} \def\mbj{\boldsymbol j} \def\mbk{\boldsymbol k} \def\mbl{\boldsymbol l} \def\mbm{\boldsymbol m} \def\mbn{\boldsymbol n} \def\mbo{\boldsymbol o} \def\mbp{\boldsymbol p} \def\mbq{\boldsymbol q} \def\mbr{\boldsymbol r} \def\mbs{\boldsymbol s} \def\mbt{\boldsymbol t} \def\mbu{\boldsymbol u} \def\mbv{\boldsymbol v} \def\mbw{\textcolor{params}{\boldsymbol w}} \def\mbx{\textcolor{input}{\boldsymbol x}} \def\mby{\boldsymbol y} \def\mbz{\boldsymbol z} \def\mbA{\boldsymbol A} \def\mbB{\boldsymbol B} \def\mbE{\boldsymbol E} \def\mbH{\boldsymbol{H}} \def\mbK{\boldsymbol{K}} \def\mbP{\boldsymbol{P}} \def\mbR{\boldsymbol{R}} \def\mbW{\textcolor{params}{\boldsymbol W}} \def\mbQ{\boldsymbol{Q}} \def\mbV{\boldsymbol{V}} \def\mbtheta{\textcolor{params}{\boldsymbol \theta}} \def\mbzero{\boldsymbol 0} \def\mbI{\boldsymbol I} \def\cF{\mathcal F} \def\cH{\mathcal H} \def\cL{\mathcal L} \def\cM{\mathcal M} \def\cN{\mathcal N} \def\cX{\mathcal X} \def\cY{\mathcal Y} \def\cU{\mathcal U} \def\bbR{\mathbb R} \def\y{\textcolor{output}{y}} $$ </div> <h3 id="table-of-spherical-harmonics">Table of Spherical Harmonics</h3> <p>Given a unit vector $(x,y,z)$ and the spherical harmonics \(Y_0^0 = \frac{1}{\sqrt{4\pi}}\)</p> <h3 id="so3-equivariant-graph-neural-network">SO(3)-Equivariant Graph Neural Network</h3>]]></content><author><name>Yuchao Lin</name></author><summary type="html"><![CDATA[$$ \definecolor{input}{rgb}{0.42, 0.55, 0.74} \definecolor{params}{rgb}{0.51,0.70,0.40} \definecolor{output}{rgb}{0.843, 0.608, 0} \def\mba{\boldsymbol a} \def\mbb{\boldsymbol b} \def\mbc{\boldsymbol c} \def\mbd{\boldsymbol d} \def\mbe{\boldsymbol e} \def\mbf{\boldsymbol f} \def\mbg{\boldsymbol g} \def\mbh{\boldsymbol h} \def\mbi{\boldsymbol i} \def\mbj{\boldsymbol j} \def\mbk{\boldsymbol k} \def\mbl{\boldsymbol l} \def\mbm{\boldsymbol m} \def\mbn{\boldsymbol n} \def\mbo{\boldsymbol o} \def\mbp{\boldsymbol p} \def\mbq{\boldsymbol q} \def\mbr{\boldsymbol r} \def\mbs{\boldsymbol s} \def\mbt{\boldsymbol t} \def\mbu{\boldsymbol u} \def\mbv{\boldsymbol v} \def\mbw{\textcolor{params}{\boldsymbol w}} \def\mbx{\textcolor{input}{\boldsymbol x}} \def\mby{\boldsymbol y} \def\mbz{\boldsymbol z} \def\mbA{\boldsymbol A} \def\mbB{\boldsymbol B} \def\mbE{\boldsymbol E} \def\mbH{\boldsymbol{H}} \def\mbK{\boldsymbol{K}} \def\mbP{\boldsymbol{P}} \def\mbR{\boldsymbol{R}} \def\mbW{\textcolor{params}{\boldsymbol W}} \def\mbQ{\boldsymbol{Q}} \def\mbV{\boldsymbol{V}} \def\mbtheta{\textcolor{params}{\boldsymbol \theta}} \def\mbzero{\boldsymbol 0} \def\mbI{\boldsymbol I} \def\cF{\mathcal F} \def\cH{\mathcal H} \def\cL{\mathcal L} \def\cM{\mathcal M} \def\cN{\mathcal N} \def\cX{\mathcal X} \def\cY{\mathcal Y} \def\cU{\mathcal U} \def\bbR{\mathbb R} \def\y{\textcolor{output}{y}} $$]]></summary></entry><entry><title type="html">What is the Drug Discovery</title><link href="https://kruskallin.github.io/2024/blog/introduction-to-drug-discovery/" rel="alternate" type="text/html" title="What is the Drug Discovery"/><published>2025-07-07T00:00:00+02:00</published><updated>2025-07-07T00:00:00+02:00</updated><id>https://kruskallin.github.io/2024/blog/introduction-to-drug-discovery</id><content type="html" xml:base="https://kruskallin.github.io/2024/blog/introduction-to-drug-discovery/"><![CDATA[<div style="display: none"> $$ \definecolor{input}{rgb}{0.42, 0.55, 0.74} \definecolor{params}{rgb}{0.51,0.70,0.40} \definecolor{output}{rgb}{0.843, 0.608, 0} \def\mba{\boldsymbol a} \def\mbb{\boldsymbol b} \def\mbc{\boldsymbol c} \def\mbd{\boldsymbol d} \def\mbe{\boldsymbol e} \def\mbf{\boldsymbol f} \def\mbg{\boldsymbol g} \def\mbh{\boldsymbol h} \def\mbi{\boldsymbol i} \def\mbj{\boldsymbol j} \def\mbk{\boldsymbol k} \def\mbl{\boldsymbol l} \def\mbm{\boldsymbol m} \def\mbn{\boldsymbol n} \def\mbo{\boldsymbol o} \def\mbp{\boldsymbol p} \def\mbq{\boldsymbol q} \def\mbr{\boldsymbol r} \def\mbs{\boldsymbol s} \def\mbt{\boldsymbol t} \def\mbu{\boldsymbol u} \def\mbv{\boldsymbol v} \def\mbw{\textcolor{params}{\boldsymbol w}} \def\mbx{\textcolor{input}{\boldsymbol x}} \def\mby{\boldsymbol y} \def\mbz{\boldsymbol z} \def\mbA{\boldsymbol A} \def\mbB{\boldsymbol B} \def\mbE{\boldsymbol E} \def\mbH{\boldsymbol{H}} \def\mbK{\boldsymbol{K}} \def\mbP{\boldsymbol{P}} \def\mbR{\boldsymbol{R}} \def\mbW{\textcolor{params}{\boldsymbol W}} \def\mbQ{\boldsymbol{Q}} \def\mbV{\boldsymbol{V}} \def\mbtheta{\textcolor{params}{\boldsymbol \theta}} \def\mbzero{\boldsymbol 0} \def\mbI{\boldsymbol I} \def\cF{\mathcal F} \def\cH{\mathcal H} \def\cL{\mathcal L} \def\cM{\mathcal M} \def\cN{\mathcal N} \def\cX{\mathcal X} \def\cY{\mathcal Y} \def\cU{\mathcal U} \def\bbR{\mathbb R} \def\y{\textcolor{output}{y}} $$ </div> <p><strong><em>Drug discovery</em></strong> is the process by which new candidate medications are discovered. Historically the drug discovery pipeline has relied on screening of large compound libraries followed by years of chemical optimization and clinical testing; success rates remain low and average costs can surpass billions of dollars. The recent advancement of machine learning contributes to substantially <strong>compressing the search space of candidate drugs</strong> and so that reduces the number of compounds that must be synthesized and the corresponding time and budget, though machine learning <strong>does not</strong> shorten the clinical phase time and regulatory requirements.</p> <h2 id="pipeline-of-drug-discovery">Pipeline of Drug Discovery</h2> <p>An industrial pipeline of drug discovery is commonly partitioned into <strong>four</strong> phases:</p> <ul> <li><em>Early drug discovery</em> begins by identifying a biological <strong><em>target</em></strong> (often a protein) whose altered activity is believed to influence a disease, and then searches chemical space for <strong><em>hits</em></strong>, compounds that modulate that target by high‑throughput robotic assays and computer‑based <strong><em>virtual screening</em></strong> with thousands to millions of molecules. Following this, medicinal chemists refine each hit’s structure to improve potency, selectivity, solubility, and preliminary safety in simple laboratory assays, yielding only a few hundred well‑characterized <strong><em>lead</em></strong> compounds.</li> <li><em>Pre-clinical phase</em> further refines and optimizes the candidate leads and conducts experiments <em>in vitro</em> (cell tests) or <em>in vivo</em> (animal tests). Compounds that fail on efficacy, bioavailability, or safety are discarded, so only several can survive and earn the data package needed to justify first‑in‑human trials.</li> <li><em>Clinical phase</em> unfolds in three numbered trials for the drugs from pre-clinical phase: <em>Phase I</em>, typically a small group of healthy volunteers, establishes a safe dose range and short‑term side‑effect profile; <em>Phase II</em>, first patients, provides the first evidence of efficacy; <em>Phase III</em>, a larger group of patients, confirms efficacy across varied populations and tracks less common adverse events, producing the complete risk–benefit dataset needed for marketing approval.</li> <li><em>Regulatory review</em> compiles all laboratory, animal, and human data into a <em>New Drug Application</em> (NDA) for small molecules or a <em>Biologics License Application</em> (BLA) for biologic therapies; agencies such as the FDA inspect raw data, and judge whether therapeutic benefit outweighs risk before granting or denying market authorization.</li> </ul> <p>This top-down model is visualized in. Note that the overall <strong>attrition of this pipeline is steep</strong> because fewer than ten percent of lead candidates survive to approval, and every failure discovered late dramatically inflates cost.</p> <h2 id="where-machine-learning-intervenes">Where Machine Learning Intervenes</h2> <p>Machine‑learning enters primarily in <strong>early drug discovery</strong> through <em>target discovery</em>, <em>hit discovery</em>, <em>hit-to-lead</em> and <em>lead optimization</em>, where data are abundant yet experimental cycles are costly and slow. Several key tasks of machine learning are included in each stage:</p> <ul> <li><strong>Target discovery</strong> identifies</li> <li><strong>Hit discovery</strong></li> <li><strong>Hit-to-lead</strong></li> <li><strong>Lead optimization</strong></li> </ul> <h2 id="scope-of-this-tutorial-series">Scope of This Tutorial Series</h2> <p>Machine learning is not a replacement for biology, organic chemistry, or clinical insight, but it already reallocates experimental effort toward the most promising hypotheses. As foundation models scale and multi‑modal data sets expand—combining cryo‑EM structures, single‑cell omics, and longitudinal patient records—the fidelity of <em>in silico</em> predictions will rise. The long‑term goal is not merely faster discovery but a virtuous cycle in which every wet‑lab measurement feeds back to train models that, in turn, suggest better experiments.</p> <p>In this tutorial</p>]]></content><author><name>Yuchao Lin</name></author><summary type="html"><![CDATA[$$ \definecolor{input}{rgb}{0.42, 0.55, 0.74} \definecolor{params}{rgb}{0.51,0.70,0.40} \definecolor{output}{rgb}{0.843, 0.608, 0} \def\mba{\boldsymbol a} \def\mbb{\boldsymbol b} \def\mbc{\boldsymbol c} \def\mbd{\boldsymbol d} \def\mbe{\boldsymbol e} \def\mbf{\boldsymbol f} \def\mbg{\boldsymbol g} \def\mbh{\boldsymbol h} \def\mbi{\boldsymbol i} \def\mbj{\boldsymbol j} \def\mbk{\boldsymbol k} \def\mbl{\boldsymbol l} \def\mbm{\boldsymbol m} \def\mbn{\boldsymbol n} \def\mbo{\boldsymbol o} \def\mbp{\boldsymbol p} \def\mbq{\boldsymbol q} \def\mbr{\boldsymbol r} \def\mbs{\boldsymbol s} \def\mbt{\boldsymbol t} \def\mbu{\boldsymbol u} \def\mbv{\boldsymbol v} \def\mbw{\textcolor{params}{\boldsymbol w}} \def\mbx{\textcolor{input}{\boldsymbol x}} \def\mby{\boldsymbol y} \def\mbz{\boldsymbol z} \def\mbA{\boldsymbol A} \def\mbB{\boldsymbol B} \def\mbE{\boldsymbol E} \def\mbH{\boldsymbol{H}} \def\mbK{\boldsymbol{K}} \def\mbP{\boldsymbol{P}} \def\mbR{\boldsymbol{R}} \def\mbW{\textcolor{params}{\boldsymbol W}} \def\mbQ{\boldsymbol{Q}} \def\mbV{\boldsymbol{V}} \def\mbtheta{\textcolor{params}{\boldsymbol \theta}} \def\mbzero{\boldsymbol 0} \def\mbI{\boldsymbol I} \def\cF{\mathcal F} \def\cH{\mathcal H} \def\cL{\mathcal L} \def\cM{\mathcal M} \def\cN{\mathcal N} \def\cX{\mathcal X} \def\cY{\mathcal Y} \def\cU{\mathcal U} \def\bbR{\mathbb R} \def\y{\textcolor{output}{y}} $$]]></summary></entry><entry><title type="html">Molecular Graphs and Geometric Graph Neural Networks</title><link href="https://kruskallin.github.io/2024/blog/molecular-property-prediction/" rel="alternate" type="text/html" title="Molecular Graphs and Geometric Graph Neural Networks"/><published>2025-07-07T00:00:00+02:00</published><updated>2025-07-07T00:00:00+02:00</updated><id>https://kruskallin.github.io/2024/blog/molecular-property-prediction</id><content type="html" xml:base="https://kruskallin.github.io/2024/blog/molecular-property-prediction/"><![CDATA[<div style="display: none"> $$ \definecolor{input}{rgb}{0.42, 0.55, 0.74} \definecolor{params}{rgb}{0.51,0.70,0.40} \definecolor{output}{rgb}{0.843, 0.608, 0} \def\mba{\boldsymbol a} \def\mbb{\boldsymbol b} \def\mbc{\boldsymbol c} \def\mbd{\boldsymbol d} \def\mbe{\boldsymbol e} \def\mbf{\boldsymbol f} \def\mbg{\boldsymbol g} \def\mbh{\boldsymbol h} \def\mbi{\boldsymbol i} \def\mbj{\boldsymbol j} \def\mbk{\boldsymbol k} \def\mbl{\boldsymbol l} \def\mbm{\boldsymbol m} \def\mbn{\boldsymbol n} \def\mbo{\boldsymbol o} \def\mbp{\boldsymbol p} \def\mbq{\boldsymbol q} \def\mbr{\boldsymbol r} \def\mbs{\boldsymbol s} \def\mbt{\boldsymbol t} \def\mbu{\boldsymbol u} \def\mbv{\boldsymbol v} \def\mbw{\textcolor{params}{\boldsymbol w}} \def\mbx{\textcolor{input}{\boldsymbol x}} \def\mby{\boldsymbol y} \def\mbz{\boldsymbol z} \def\mbA{\boldsymbol A} \def\mbB{\boldsymbol B} \def\mbE{\boldsymbol E} \def\mbH{\boldsymbol{H}} \def\mbK{\boldsymbol{K}} \def\mbP{\boldsymbol{P}} \def\mbR{\boldsymbol{R}} \def\mbW{\textcolor{params}{\boldsymbol W}} \def\mbQ{\boldsymbol{Q}} \def\mbV{\boldsymbol{V}} \def\mbtheta{\textcolor{params}{\boldsymbol \theta}} \def\mbzero{\boldsymbol 0} \def\mbI{\boldsymbol I} \def\cF{\mathcal F} \def\cH{\mathcal H} \def\cL{\mathcal L} \def\cM{\mathcal M} \def\cN{\mathcal N} \def\cX{\mathcal X} \def\cY{\mathcal Y} \def\cU{\mathcal U} \def\bbR{\mathbb R} \def\y{\textcolor{output}{y}} $$ </div>]]></content><author><name>Yuchao Lin</name></author><summary type="html"><![CDATA[$$ \definecolor{input}{rgb}{0.42, 0.55, 0.74} \definecolor{params}{rgb}{0.51,0.70,0.40} \definecolor{output}{rgb}{0.843, 0.608, 0} \def\mba{\boldsymbol a} \def\mbb{\boldsymbol b} \def\mbc{\boldsymbol c} \def\mbd{\boldsymbol d} \def\mbe{\boldsymbol e} \def\mbf{\boldsymbol f} \def\mbg{\boldsymbol g} \def\mbh{\boldsymbol h} \def\mbi{\boldsymbol i} \def\mbj{\boldsymbol j} \def\mbk{\boldsymbol k} \def\mbl{\boldsymbol l} \def\mbm{\boldsymbol m} \def\mbn{\boldsymbol n} \def\mbo{\boldsymbol o} \def\mbp{\boldsymbol p} \def\mbq{\boldsymbol q} \def\mbr{\boldsymbol r} \def\mbs{\boldsymbol s} \def\mbt{\boldsymbol t} \def\mbu{\boldsymbol u} \def\mbv{\boldsymbol v} \def\mbw{\textcolor{params}{\boldsymbol w}} \def\mbx{\textcolor{input}{\boldsymbol x}} \def\mby{\boldsymbol y} \def\mbz{\boldsymbol z} \def\mbA{\boldsymbol A} \def\mbB{\boldsymbol B} \def\mbE{\boldsymbol E} \def\mbH{\boldsymbol{H}} \def\mbK{\boldsymbol{K}} \def\mbP{\boldsymbol{P}} \def\mbR{\boldsymbol{R}} \def\mbW{\textcolor{params}{\boldsymbol W}} \def\mbQ{\boldsymbol{Q}} \def\mbV{\boldsymbol{V}} \def\mbtheta{\textcolor{params}{\boldsymbol \theta}} \def\mbzero{\boldsymbol 0} \def\mbI{\boldsymbol I} \def\cF{\mathcal F} \def\cH{\mathcal H} \def\cL{\mathcal L} \def\cM{\mathcal M} \def\cN{\mathcal N} \def\cX{\mathcal X} \def\cY{\mathcal Y} \def\cU{\mathcal U} \def\bbR{\mathbb R} \def\y{\textcolor{output}{y}} $$]]></summary></entry><entry><title type="html">Data and Representation in Machine Learning Drug Discovery</title><link href="https://kruskallin.github.io/2024/blog/molecular-representations/" rel="alternate" type="text/html" title="Data and Representation in Machine Learning Drug Discovery"/><published>2025-07-07T00:00:00+02:00</published><updated>2025-07-07T00:00:00+02:00</updated><id>https://kruskallin.github.io/2024/blog/molecular-representations</id><content type="html" xml:base="https://kruskallin.github.io/2024/blog/molecular-representations/"><![CDATA[<div style="display: none"> $$ \definecolor{input}{rgb}{0.42, 0.55, 0.74} \definecolor{params}{rgb}{0.51,0.70,0.40} \definecolor{output}{rgb}{0.843, 0.608, 0} \def\mba{\boldsymbol a} \def\mbb{\boldsymbol b} \def\mbc{\boldsymbol c} \def\mbd{\boldsymbol d} \def\mbe{\boldsymbol e} \def\mbf{\boldsymbol f} \def\mbg{\boldsymbol g} \def\mbh{\boldsymbol h} \def\mbi{\boldsymbol i} \def\mbj{\boldsymbol j} \def\mbk{\boldsymbol k} \def\mbl{\boldsymbol l} \def\mbm{\boldsymbol m} \def\mbn{\boldsymbol n} \def\mbo{\boldsymbol o} \def\mbp{\boldsymbol p} \def\mbq{\boldsymbol q} \def\mbr{\boldsymbol r} \def\mbs{\boldsymbol s} \def\mbt{\boldsymbol t} \def\mbu{\boldsymbol u} \def\mbv{\boldsymbol v} \def\mbw{\textcolor{params}{\boldsymbol w}} \def\mbx{\textcolor{input}{\boldsymbol x}} \def\mby{\boldsymbol y} \def\mbz{\boldsymbol z} \def\mbA{\boldsymbol A} \def\mbB{\boldsymbol B} \def\mbE{\boldsymbol E} \def\mbH{\boldsymbol{H}} \def\mbK{\boldsymbol{K}} \def\mbP{\boldsymbol{P}} \def\mbR{\boldsymbol{R}} \def\mbW{\textcolor{params}{\boldsymbol W}} \def\mbQ{\boldsymbol{Q}} \def\mbV{\boldsymbol{V}} \def\mbtheta{\textcolor{params}{\boldsymbol \theta}} \def\mbzero{\boldsymbol 0} \def\mbI{\boldsymbol I} \def\cF{\mathcal F} \def\cH{\mathcal H} \def\cL{\mathcal L} \def\cM{\mathcal M} \def\cN{\mathcal N} \def\cX{\mathcal X} \def\cY{\mathcal Y} \def\cU{\mathcal U} \def\bbR{\mathbb R} \def\y{\textcolor{output}{y}} $$ </div> <p>A molecular representation is a formal mapping that converts the physical reality of a chemical compound into a data structure suitable for computation. An ideal representation preserves the information needed to predict relevant observables—reactivity, binding affinity, synthetic accessibility—while discarding superfluous detail so that downstream models remain tractable. In practice, no single encoding is universally optimal. Line notations such as SMILES are compact and easily tokenized, graph structures capture connectivity and stereochemistry, and three‑dimensional coordinates expose the spatial degrees of freedom that dominate non‑covalent interactions. Modern machine‑learning pipelines move fluidly among these layers, translating one form into another as the task demands.</p> <h2 id="basic-molecular-representations">Basic Molecular Representations</h2> <h3 id="smiles">SMILES</h3> <p>The SMILES specification encodes a molecule as a sequence of ASCII tokens that follow depth‑first traversal of its molecular graph. Atoms appear as element symbols surrounded by optional brackets that annotate isotope, charge, stereochemistry, and hydrogen count. Single bonds are implicit; double, triple, and aromatic bonds use the symbols <code class="language-plaintext highlighter-rouge">=</code>, <code class="language-plaintext highlighter-rouge">#</code>, and <code class="language-plaintext highlighter-rouge">:</code>. Branches are enclosed in parentheses, and ring closures are indicated by numerical indices that mark the two atoms joined. Canonicalization algorithms reorder traversal deterministically, giving each structure a unique “canonical SMILES” that enables rapid hashing and duplicate removal. The representation is concise—benzene reduces to the six‑character string <code class="language-plaintext highlighter-rouge">"c1ccccc1"</code>—and readily tokenized for language models, yet its one‑dimensional nature obscures conformational information and allows syntactically valid but chemically impossible strings. Augmentations such as random SMILES enumeration and SELFIES introduce robustness by ensuring all generated strings decode to valid molecules.</p> <h3 id="molecular-topological-graph">Molecular Topological Graph</h3> <p>A topological graph models a molecule as an undirected multigraph $G=(V,E)$ where vertices correspond to atoms and edges to covalent bonds. Vertex attributes typically include atomic number, formal charge, aromaticity, degree, and hybridization state; edge attributes describe bond order, conjugation, and stereochemical parity. The graph abstraction preserves the combinatorial chemistry that governs reaction transforms and scaffold hopping while remaining invariant to translation and rotation. Algorithms for subgraph matching, cycle detection, and fingerprinting operate naturally on this structure, and it forms the backbone input for message‑passing neural networks. Chirality and cis–trans isomerism require additional edge or face labels so that enantiomers do not collapse into the same topology.</p> <h3 id="3d-molecular-structure">3D Molecular Structure</h3> <p>Three‑dimensional structure specifies Cartesian coordinates for each atom, optionally accompanied by partial charges, atomic radii, or multipole moments. A single molecule usually admits a thermodynamic ensemble of conformers whose populations depend on temperature, solvent, and steric constraints. Quantum chemistry and molecular‑mechanics force fields supply potential energies that rank these conformers, making geometry a central variable in docking, binding free‑energy estimation, and property prediction tasks sensitive to shape and electrostatics. Coordinate files appear in formats such as PDB, SDF, and XYZ, each with conventions for unit cell, connectivity, and metadata. Accuracy in 3D comes at computational cost: learning algorithms must be equivariant to rotations and, for flexible molecules, robust to continuous deformations across conformational space.</p>]]></content><author><name>Yuchao Lin</name></author><summary type="html"><![CDATA[$$ \definecolor{input}{rgb}{0.42, 0.55, 0.74} \definecolor{params}{rgb}{0.51,0.70,0.40} \definecolor{output}{rgb}{0.843, 0.608, 0} \def\mba{\boldsymbol a} \def\mbb{\boldsymbol b} \def\mbc{\boldsymbol c} \def\mbd{\boldsymbol d} \def\mbe{\boldsymbol e} \def\mbf{\boldsymbol f} \def\mbg{\boldsymbol g} \def\mbh{\boldsymbol h} \def\mbi{\boldsymbol i} \def\mbj{\boldsymbol j} \def\mbk{\boldsymbol k} \def\mbl{\boldsymbol l} \def\mbm{\boldsymbol m} \def\mbn{\boldsymbol n} \def\mbo{\boldsymbol o} \def\mbp{\boldsymbol p} \def\mbq{\boldsymbol q} \def\mbr{\boldsymbol r} \def\mbs{\boldsymbol s} \def\mbt{\boldsymbol t} \def\mbu{\boldsymbol u} \def\mbv{\boldsymbol v} \def\mbw{\textcolor{params}{\boldsymbol w}} \def\mbx{\textcolor{input}{\boldsymbol x}} \def\mby{\boldsymbol y} \def\mbz{\boldsymbol z} \def\mbA{\boldsymbol A} \def\mbB{\boldsymbol B} \def\mbE{\boldsymbol E} \def\mbH{\boldsymbol{H}} \def\mbK{\boldsymbol{K}} \def\mbP{\boldsymbol{P}} \def\mbR{\boldsymbol{R}} \def\mbW{\textcolor{params}{\boldsymbol W}} \def\mbQ{\boldsymbol{Q}} \def\mbV{\boldsymbol{V}} \def\mbtheta{\textcolor{params}{\boldsymbol \theta}} \def\mbzero{\boldsymbol 0} \def\mbI{\boldsymbol I} \def\cF{\mathcal F} \def\cH{\mathcal H} \def\cL{\mathcal L} \def\cM{\mathcal M} \def\cN{\mathcal N} \def\cX{\mathcal X} \def\cY{\mathcal Y} \def\cU{\mathcal U} \def\bbR{\mathbb R} \def\y{\textcolor{output}{y}} $$]]></summary></entry></feed>